{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9456a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      " LOADING DATA\n",
      "====================================\n",
      "Train Shape: (181507, 279)\n",
      "Test Shape : (77789, 278)\n",
      "\n",
      "Preview:\n",
      "       id      full_sq      life_sq      floor product_type   sub_area  \\\n",
      "0  106299    -0.280658    50.127224  10.970619       Type_A   Area_380   \n",
      "1  125559  2129.178675  3438.561939   6.282464       Type_B  Area_1375   \n",
      "2  204969    44.255548   -15.662341   0.894701       Type_A   Area_272   \n",
      "3  248026  2622.821354  1373.058212  15.971260       Type_B  Area_1750   \n",
      "4   51881    52.877556    12.392606   2.876796       Type_A  Area_1773   \n",
      "\n",
      "   green_zone_part  indust_part  children_preschool  \\\n",
      "0         0.099506     0.222095         6255.829172   \n",
      "1         0.588042     0.157862         7391.899017   \n",
      "2         0.074837     0.266754         5658.091711   \n",
      "3         0.850289     0.393528        13262.734984   \n",
      "4         0.070450     0.238344         3842.416286   \n",
      "\n",
      "   preschool_education_centers_raion  ...  office_sqm_5000_log  \\\n",
      "0                           4.076307  ...            13.558453   \n",
      "1                           7.608949  ...            15.797908   \n",
      "2                           4.054293  ...            13.586732   \n",
      "3                           6.832598  ...            13.761426   \n",
      "4                           5.030503  ...            14.068752   \n",
      "\n",
      "   trc_sqm_5000_log  sport_count_5000_log  life_full_ratio  cafe_density_5000  \\\n",
      "0         14.688605              4.046719      -178.606113          16.135148   \n",
      "1         15.293149              3.805732         1.614971         166.763131   \n",
      "2         14.162732              3.987516        -0.353907          12.076017   \n",
      "3         14.881627              4.765929         0.523504         184.953625   \n",
      "4         14.302936              4.114335         0.234364          14.630758   \n",
      "\n",
      "   high_floor  large_apartment rooms_inferred  living_efficiency   price_doc  \n",
      "0           0                0            3.0          69.684824    6.452158  \n",
      "1           0                1          229.0           1.614213  101.661749  \n",
      "2           0                0            1.0          -0.346087    6.257546  \n",
      "3           1                1           91.0           0.523305   90.592523  \n",
      "4           0                0            1.0           0.230014    8.486698  \n",
      "\n",
      "[5 rows x 279 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "print(\"\\n====================================\")\n",
    "print(\" LOADING DATA\")\n",
    "print(\"====================================\")\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Train Shape: {train.shape}\")\n",
    "print(f\"Test Shape : {test.shape}\")\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1452e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "ðŸ“Œ MISSING VALUE ANALYSIS\n",
      "====================================\n",
      "\n",
      "Missing Values Summary:\n",
      "                              Missing Count  Missing %\n",
      "office_sqm_5000_log                   11249   6.197557\n",
      "trc_sqm_5000_log                       7216   3.975604\n",
      "full_all_log                           4023   2.216443\n",
      "sport_count_5000_log                   2351   1.295267\n",
      "area_m_log                               28   0.015426\n",
      "...                                     ...        ...\n",
      "culture_objects_top_25_raion              0   0.000000\n",
      "shopping_centers_raion                    0   0.000000\n",
      "office_raion                              0   0.000000\n",
      "thermal_power_plant_raion                 0   0.000000\n",
      "mosque_count_5000                         0   0.000000\n",
      "\n",
      "[279 rows x 2 columns]\n",
      "\n",
      "Features with > 40% missing values:\n",
      "Empty DataFrame\n",
      "Columns: [Missing Count, Missing %]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Missing Values Analysis\n",
    "# =========================================================\n",
    "print(\"\\n====================================\")\n",
    "print(\"ðŸ“Œ MISSING VALUE ANALYSIS\")\n",
    "print(\"====================================\")\n",
    "\n",
    "missing_vals = train.isna().sum()\n",
    "missing_percent = (missing_vals / len(train)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    \"Missing Count\": missing_vals,\n",
    "    \"Missing %\": missing_percent\n",
    "}).sort_values(by=\"Missing %\", ascending=False)\n",
    "\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "print(missing_df)\n",
    "\n",
    "print(\"\\nFeatures with > 40% missing values:\")\n",
    "print(missing_df[missing_df[\"Missing %\"] > 40]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e267c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "ðŸ“Œ TARGET VARIABLE ANALYSIS\n",
      "====================================\n",
      "Target Stats:\n",
      "count    181507.000000\n",
      "mean         14.845599\n",
      "std          21.533138\n",
      "min           0.392328\n",
      "25%           5.303449\n",
      "50%           7.186257\n",
      "75%          11.781645\n",
      "max         109.864990\n",
      "Name: price_doc, dtype: float64\n",
      "\n",
      "Skewness of Target: 2.7868141322285016\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 2. Target Variable Analysis (Check Imbalance)\n",
    "# =========================================================\n",
    "\n",
    "TARGET = \"price_doc\"    # â† change according to your dataset!\n",
    "\n",
    "if TARGET in train.columns:\n",
    "    print(\"\\n====================================\")\n",
    "    print(\"ðŸ“Œ TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"====================================\")\n",
    "\n",
    "    print(f\"Target Stats:\\n{train[TARGET].describe()}\")\n",
    "    \n",
    "    # Check if skewed\n",
    "    print(\"\\nSkewness of Target:\", train[TARGET].skew())\n",
    "\n",
    "   \n",
    "\n",
    "else:\n",
    "    print(\"\\nâš  ALERT: No 'target' column found. Set TARGET variable correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f602239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "ðŸ“Œ BASIC STATISTICAL SUMMARY\n",
      "====================================\n",
      "                      count           mean           std           min  \\\n",
      "id                 181507.0  129814.082570  74904.378967      2.000000   \n",
      "full_sq            181507.0     446.686938   1106.420198    -60.517111   \n",
      "life_sq            181507.0     595.833855   1511.699330    -98.221082   \n",
      "floor              181507.0      11.597576     14.755608     -0.645939   \n",
      "green_zone_part    181507.0       0.240568      0.213208     -0.013725   \n",
      "...                     ...            ...           ...           ...   \n",
      "high_floor         181507.0       0.166980      0.372959      0.000000   \n",
      "large_apartment    181507.0       0.174302      0.379370      0.000000   \n",
      "rooms_inferred     181507.0      39.677136    100.601553      1.000000   \n",
      "living_efficiency  181507.0       2.081635     87.664118 -15137.795627   \n",
      "price_doc          181507.0      14.845599     21.533138      0.392328   \n",
      "\n",
      "                            25%            50%            75%            max  \n",
      "id                 65091.500000  129857.000000  194675.500000  259296.000000  \n",
      "full_sq               36.527739      57.850524      88.740084    5367.642700  \n",
      "life_sq               16.459273      43.590090      90.921327    7566.475386  \n",
      "floor                  3.846373       6.776766      11.624073      77.659678  \n",
      "green_zone_part        0.065424       0.167194       0.370981       0.867485  \n",
      "...                         ...            ...            ...            ...  \n",
      "high_floor             0.000000       0.000000       0.000000       1.000000  \n",
      "large_apartment        0.000000       0.000000       0.000000       1.000000  \n",
      "rooms_inferred         1.000000       2.000000       6.000000     504.000000  \n",
      "living_efficiency      0.242012       0.710820       1.471545   18533.764079  \n",
      "price_doc              5.303449       7.186257      11.781645     109.864990  \n",
      "\n",
      "[264 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 3. Basic Statistical Summary\n",
    "# =========================================================\n",
    "print(\"\\n====================================\")\n",
    "print(\"ðŸ“Œ BASIC STATISTICAL SUMMARY\")\n",
    "print(\"====================================\")\n",
    "\n",
    "print(train.describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c4f783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "ðŸ“Œ CORRELATION ANALYSIS\n",
      "====================================\n",
      "\n",
      "Top 20 correlations with Target:\n",
      "price_doc                     1.000000\n",
      "large_apartment               0.737889\n",
      "full_sq                       0.669290\n",
      "leisure_count_1000            0.659987\n",
      "leisure_count_500             0.659467\n",
      "cafe_count_500_price_4000     0.654416\n",
      "trc_sqm_500                   0.651915\n",
      "cafe_count_1000_price_high    0.651016\n",
      "mosque_count_500              0.649162\n",
      "cafe_count_500_price_high     0.648583\n",
      "cafe_count_500_price_1000     0.646469\n",
      "leisure_count_1500            0.640853\n",
      "cafe_count_1000_price_2500    0.638695\n",
      "cafe_count_1000_price_4000    0.638275\n",
      "office_sqm_1000               0.637163\n",
      "cafe_count_500_price_1500     0.636903\n",
      "cafe_count_1500_price_high    0.636007\n",
      "office_count_500              0.635852\n",
      "big_church_count_1000         0.635598\n",
      "big_church_count_500          0.635320\n",
      "Name: price_doc, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 4. Correlation Matrix\n",
    "# =========================================================\n",
    "print(\"\\n====================================\")\n",
    "print(\"ðŸ“Œ CORRELATION ANALYSIS\")\n",
    "print(\"====================================\")\n",
    "\n",
    "# Only numeric features\n",
    "num_cols = train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "corr_matrix = train[num_cols].corr()\n",
    "\n",
    "# Show top correlated features with target\n",
    "if TARGET in num_cols:\n",
    "    print(\"\\nTop 20 correlations with Target:\")\n",
    "    print(corr_matrix[TARGET].sort_values(ascending=False).head(20))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e59cc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Target column: 'price_doc'\n"
     ]
    }
   ],
   "source": [
    "# Identify target column (adjust based on your data)\n",
    "target_col = train.columns[-1]  # Assuming last column is target\n",
    "print(f\"âœ“ Target column: '{target_col}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1486bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Features shape: (181507, 277)\n",
      "âœ“ Target shape: (181507,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = train.drop(columns=[target_col])\n",
    "y = train[target_col]\n",
    "\n",
    "\n",
    "# Drop ID column if present (usually first column)\n",
    "if 'id' in X.columns.str.lower():\n",
    "    id_col = [col for col in X.columns if 'id' in col.lower()][0]\n",
    "    X = X.drop(columns=[id_col])\n",
    "    test_df = test.drop(columns=[id_col])\n",
    "\n",
    "print(f\"\\nâœ“ Features shape: {X.shape}\")\n",
    "print(f\"âœ“ Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a68456b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: PREPROCESSING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "--- Feature Types ---\n",
      "Numerical: 255\n",
      "Categorical: 15\n",
      "\n",
      "--- Handling Missing Values ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_16352\\2365127932.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(median_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Imputed 'area_m_log' with median: 16.04\n",
      "  âœ“ Imputed 'raion_popul_log' with median: 11.33\n",
      "  âœ“ Imputed 'full_all_log' with median: 11.48\n",
      "  âœ“ Imputed 'office_sqm_5000_log' with median: 13.95\n",
      "  âœ“ Imputed 'trc_sqm_5000_log' with median: 14.15\n",
      "  âœ“ Imputed 'sport_count_5000_log' with median: 4.16\n",
      "\n",
      "--- Encoding Categorical Features ---\n",
      "\n",
      "  One-hot encoding 14 features...\n",
      "  Label encoding 1 features...\n",
      "\n",
      "âœ“ Final feature shape: (181507, 273)\n",
      "\n",
      "--- Feature Engineering ---\n",
      "\n",
      "--- Scaling Features ---\n",
      "âœ“ Features scaled using RobustScaler\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: PREPROCESSING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def preprocess_data(train, test, target_col):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate target\n",
    "    X_train = train.drop(columns=[target_col])\n",
    "    y_train = train[target_col]\n",
    "    X_test = test.copy()\n",
    "    \n",
    "    # Drop ID column\n",
    "    id_cols = [col for col in X_train.columns if 'id' in col.lower()]\n",
    "    for col in id_cols:\n",
    "        X_train = X_train.drop(columns=[col])\n",
    "        if col in X_test.columns:\n",
    "            X_test = X_test.drop(columns=[col])\n",
    "    \n",
    "    print(\"\\n--- Feature Types ---\")\n",
    "    numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numerical: {len(numerical_features)}\")\n",
    "    print(f\"Categorical: {len(categorical_features)}\")\n",
    "\n",
    "\n",
    "    # ========================================================================\n",
    "    # HANDLE MISSING VALUES\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- Handling Missing Values ---\")\n",
    "    \n",
    "    # Numerical features: impute with median\n",
    "    for col in numerical_features:\n",
    "        if X_train[col].isnull().sum() > 0:\n",
    "            median_val = X_train[col].median()\n",
    "            X_train[col].fillna(median_val, inplace=True)\n",
    "            X_test[col].fillna(median_val, inplace=True)\n",
    "            print(f\"  âœ“ Imputed '{col}' with median: {median_val:.2f}\")\n",
    "    \n",
    "    # Categorical features: impute with mode\n",
    "    for col in categorical_features:\n",
    "        if X_train[col].isnull().sum() > 0:\n",
    "            mode_val = X_train[col].mode()[0]\n",
    "            X_train[col].fillna(mode_val, inplace=True)\n",
    "            X_test[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"  âœ“ Imputed '{col}' with mode: {mode_val}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ========================================================================\n",
    "    # HANDLE CATEGORICAL FEATURES\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- Encoding Categorical Features ---\")\n",
    "    \n",
    "    if len(categorical_features) > 0:\n",
    "        # Check cardinality\n",
    "        high_cardinality = []\n",
    "        low_cardinality = []\n",
    "        for col in categorical_features:\n",
    "            unique_count = X_train[col].nunique()\n",
    "            if unique_count > 10:\n",
    "                high_cardinality.append(col)\n",
    "            else:\n",
    "                low_cardinality.append(col)\n",
    "        \n",
    "        # One-hot encode low cardinality features\n",
    "        if len(low_cardinality) > 0:\n",
    "            print(f\"\\n  One-hot encoding {len(low_cardinality)} features...\")\n",
    "            X_train = pd.get_dummies(X_train, columns=low_cardinality, drop_first=True)\n",
    "            X_test = pd.get_dummies(X_test, columns=low_cardinality, drop_first=True)\n",
    "        \n",
    "        # Label encode high cardinality features\n",
    "        if len(high_cardinality) > 0:\n",
    "            print(f\"  Label encoding {len(high_cardinality)} features...\")\n",
    "            for col in high_cardinality:\n",
    "                le = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "                X_train[col] = le.fit_transform(X_train[[col]].astype(str)).ravel()\n",
    "                X_test[col] = le.transform(X_test[[col]].astype(str)).ravel()\n",
    "    \n",
    "    # Align columns between train and test\n",
    "    missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0\n",
    "    \n",
    "    X_test = X_test[X_train.columns]\n",
    "    \n",
    "    print(f\"\\nâœ“ Final feature shape: {X_train.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    # ========================================================================\n",
    "    # FEATURE ENGINEERING\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- Feature Engineering ---\")\n",
    "    \n",
    "    # Get numerical columns after encoding\n",
    "    numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Create polynomial features for top correlated features with target\n",
    "    if len(numerical_cols) > 0 and len(numerical_cols) <= 20:\n",
    "        # Calculate correlations\n",
    "        temp_df = X_train.copy()\n",
    "        temp_df['target'] = y_train\n",
    "        correlations = temp_df.corr()['target'].drop('target').abs().sort_values(ascending=False)\n",
    "        \n",
    "        # Take top 3 features\n",
    "        top_features = correlations.head(3).index.tolist()\n",
    "        \n",
    "        for i, feat in enumerate(top_features[:2]):\n",
    "            # Squared term\n",
    "            X_train[f'{feat}_squared'] = X_train[feat] ** 2\n",
    "            X_test[f'{feat}_squared'] = X_test[feat] ** 2\n",
    "            print(f\"  âœ“ Created '{feat}_squared'\")\n",
    "        \n",
    "        # Interaction between top 2\n",
    "        if len(top_features) >= 2:\n",
    "            feat1, feat2 = top_features[0], top_features[1]\n",
    "            X_train[f'{feat1}_{feat2}_interaction'] = X_train[feat1] * X_train[feat2]\n",
    "            X_test[f'{feat1}_{feat2}_interaction'] = X_test[feat1] * X_test[feat2]\n",
    "            print(f\"  âœ“ Created interaction: {feat1} Ã— {feat2}\")\n",
    "\n",
    "\n",
    "\n",
    "            # ========================================================================\n",
    "    # SCALING\n",
    "    # ========================================================================\n",
    "    print(\"\\n--- Scaling Features ---\")\n",
    "    \n",
    "    scaler = RobustScaler()  # Robust to outliers\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    print(\"âœ“ Features scaled using RobustScaler\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train\n",
    "\n",
    "# Preprocess data\n",
    "X_processed, X_test_processed, y_processed = preprocess_data(train, test, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d69cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: GRADIENT BOOSTING REGRESSOR\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: GRADIENT BOOSTING REGRESSOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==========================================\n",
    "# BASE MODEL (check baseline performance)\n",
    "# ==========================================\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "scores = cross_val_score(gbr, X_processed, y_processed, cv=5, scoring='neg_root_mean_squared_error')\n",
    "print(f\"Baseline CV RMSE: {-scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d3d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle submission saved: regression_tree_random_forest_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 2. Generate Kaggle Submission\n",
    "# ===============================\n",
    "# Predict on test set\n",
    "y_test_pred =gbr.predict(X_test_processed)\n",
    "\n",
    "# Prepare submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],   # Assuming test.csv has 'id' column\n",
    "    'price_doc': y_test_pred\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv(\"gradient_boosting_regressor_submission.csv\", index=False)\n",
    "print(\"Kaggle submission saved: gradient_boosting_regressor_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# HYPERPARAMETER TUNING (GridSearch)\n",
    "# ==========================================\n",
    "param_grid_gbr = {\n",
    "    \"n_estimators\": [200, 500, 800],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "# 3*3*3*2*3 = 324 combinations\n",
    "#  5*324=1620 fits with cv=5\n",
    "\n",
    "grid_gbr = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid_gbr,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_gbr.fit(X_processed, y_processed)\n",
    "\n",
    "print(\"\\nBest Gradient Boosting RMSE:\", -grid_gbr.best_score_)\n",
    "print(\"Best Params:\", grid_gbr.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TRAIN BEST MODEL ON FULL TRAINING SET\n",
    "# ==========================================\n",
    "best_gbr = grid_gbr.best_estimator_\n",
    "best_gbr.fit(X_processed, y_processed)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# GENERATE TEST PREDICTIONS (for Kaggle)\n",
    "# ==========================================\n",
    "gbr_preds = best_gbr.predict(X_test_processed)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],   # Change to correct ID column name\n",
    "    \"price_doc\": gbr_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"gbr_submission.csv\", index=False)\n",
    "print(\"\\nâœ“ Gradient Boosting submission file created: gbr_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
